{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data\n",
    "\n",
    "I chose to gather tweets surrounding the 2022 IN-01 congressional district race to analyze political Twitter for bot activity. The reason that I chose such a specific scope for this project is because I thought that such a specific event would have less automatic monitoring than something like a presidential election. For three weeks, I compiled a dataset of tweets mentioning four distinct topics: Frank Mrvan (D), Jennifer-Ruth Green (R), IN-01, and NWI Times (the leading news source for Northwest Indiana). This notebook shows step 1 of the overall process, which is to gather tweets using the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary step, I needed to sign up for a Twitter Developer account and recieve my bearer token, which is necessary for any Twitter API query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to create a generalized function that can return a list of tweets from a specific query. This function pulls batches of 10 tweets at a time from a query and returns them as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchTwitter(query, tweet_fields, user_fields, next_token, bearer_token):\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "\n",
    "    # Checks if another batch of tweets can be pulled from query\n",
    "    if next_token is not None:\n",
    "        url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&{tweet_fields}&{user_fields}&next_token={next_token}\"\n",
    "    else:\n",
    "        url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&{tweet_fields}&{user_fields}\"\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    # Checks for search limit and pauses for an extended time\n",
    "    if response.status_code == 429:\n",
    "        print(\"Request limit reached.\")\n",
    "        time.sleep(secs=1800)\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "\n",
    "    # Returns a JSON object\n",
    "    return response.json()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Twitter Developer student account holder, my workflow was messy through this process. I could only pull out roughly 4,000 tweets every 30 minutes, and my account was limited to tweets from the past seven days. These hinderances made for a less-than-optimal data gathering process, as I had to time my queries to be around the same time on Sunday nights for three weeks straight."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I had to determine the queries that I would use for each of the four topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrvan_query = '(\"Frank Mrvan\" OR Mrvan OR @RepMrvan) -is:retweet'\n",
    "jrg_query = '(\"Jennifer-Ruth Green\" OR JRG OR @JenRuthGreen) -is:retweet'\n",
    "in_query = '(IN01 OR \"IN-01\") -is:retweet'\n",
    "nwi_query = '(@nwi) -is:retweet'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries would pull the fields below from each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fields = \"\"\"\n",
    "    tweet.fields=\n",
    "    id,text,edit_history_tweet_ids,attachments,author_id,\n",
    "    created_at,entities,in_reply_to_user_id,\n",
    "    possibly_sensitive,public_metrics,source\n",
    "\"\"\"\n",
    "user_fields = \"\"\"\n",
    "    user.fields=\n",
    "    name,username,created_at,description,entities,\n",
    "    location,pinned_tweet_id,protected,public_metrics,verified\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query search, the query runs until either (a) the query is successfully completed and no more tweets need to be pulled or (b) the soft limit for the query has been reached. If the soft limit is reached, the searchTwitter function pauses for an extended time until the soft limit expires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "json_results = searchTwitter(\n",
    "    query, \n",
    "    tweet_fields,\n",
    "    user_fields,\n",
    "    None,\n",
    "    bearer_token\n",
    ")\n",
    "next_token = json_results[\"meta\"][\"next_token\"]\n",
    "results.append(json_results)\n",
    "\n",
    "while next_token:\n",
    "    json_results = searchTwitter(\n",
    "        query, \n",
    "        tweet_fields,\n",
    "        user_fields,\n",
    "        next_token,\n",
    "        bearer_token\n",
    "    )\n",
    "    next_token = json_results[\"meta\"][\"next_token\"]\n",
    "    results.append(json_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the tweet information is stored in a list of paginated results that needs to be reorganized into a single list of tweets. The list of tweets is then converted into a JSON object that can be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for page in results:\n",
    "    for item in page[\"data\"]:\n",
    "        tweets.append(item)\n",
    "with open('query_type.json', 'w') as f:\n",
    "    json.dump(tweets, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process was repeated on 3 different occasions for each of the 4 queries. The resulting JSON files can be found in the `data` subdirectory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 17:54:22) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c15b339058f8b70542acf75ffcaf47d4bb065aad2d91b68f67d6871ef81ef96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
